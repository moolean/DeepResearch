# DeepResearch Environment Configuration
# Copy this file to .env and fill in your actual values
# Run: cp .env.example .env

# =============================================================================
# TORCH/NCCL Configuration (for multi-GPU setups)
# =============================================================================
TORCHDYNAMO_VERBOSE=1
TORCHDYNAMO_DISABLE=1
NCCL_IB_TC=16
NCCL_IB_SL=5
NCCL_IB_GID_INDEX=3
NCCL_SOCKET_IFNAME=eth
NCCL_DEBUG=INFO
NCCL_IB_HCA=mlx5
NCCL_IB_TIMEOUT=22
NCCL_IB_QPS_PER_CONNECTION=8
NCCL_MIN_NCHANNELS=4
NCCL_NET_PLUGIN=none
GLOO_SOCKET_IFNAME=eth0

# =============================================================================
# DeepResearch Configuration
# =============================================================================
QWEN_DOC_PARSER_USE_IDP=false
QWEN_IDP_ENABLE_CSI=false
NLP_WEB_SEARCH_ONLY_CACHE=false
NLP_WEB_SEARCH_ENABLE_READPAGE=false
NLP_WEB_SEARCH_ENABLE_SFILTER=false
QWEN_SEARCH_ENABLE_CSI=false
SPECIAL_CODE_MODE=false
PYTHONDONTWRITEBYTECODE=1

# =============================================================================
# Model and Inference Hyperparameters
# =============================================================================
MODEL_PATH=/your/model/path
DATASET=your_dataset_name
OUTPUT_PATH=/your/output/path
ROLLOUT_COUNT=3
TEMPERATURE=0.85
PRESENCE_PENALTY=1.1
MAX_WORKERS=30

# =============================================================================
# Inference API Configuration (Optional - for API-based inference)
# =============================================================================
# If you want to use a remote API for inference instead of local VLLM servers
# Set USE_REMOTE_API=true and configure the API endpoint and key
USE_REMOTE_API=false
INFERENCE_API_BASE=http://127.0.0.1:6001/v1
INFERENCE_API_KEY=EMPTY
# Use requests-based middleware instead of openai library (optional)
# Set to true if you want to use the lightweight HTTP client
USE_OPENAI_MIDDLEWARE=true
# =============================================================================
# Tool Selection Configuration
# =============================================================================
# Comma-separated list of tools to enable for inference
# Available tools: search, visit, google_scholar, PythonInterpreter, parse_file
# Example: ENABLED_TOOLS=search,visit,PythonInterpreter
ENABLED_TOOLS=search,visit,google_scholar,PythonInterpreter,parse_file

# =============================================================================
# API Keys and External Services
# =============================================================================

# Serper API for web search and Google Scholar
# Get your key from: https://serper.dev/
SERPER_KEY_ID=your_key

# Jina API for web page reading
# Get your key from: https://jina.ai/
JINA_API_KEYS=your_key

# Enable direct URL fetching (bypasses Jina API)
# Set to true to fetch URLs directly using httpx instead of Jina
# Default: false (uses Jina API)
USE_DIRECT_FETCH=false

# Summary model API (OpenAI-compatible) for page summarization
# Get your key from: https://platform.openai.com/
API_KEY=your_key
API_BASE=your_api_base
SUMMARY_MODEL_NAME=your_summary_model_name

# Dashscope API for file parsing (PDF, Office, etc.)
# Get your key from: https://dashscope.aliyun.com/
# Supports: qwen-omni-turbo, qwen-plus-latest
DASHSCOPE_API_KEY=your_key
DASHSCOPE_API_BASE=your_api_base
VIDEO_MODEL_NAME=your_video_model_name
VIDEO_ANALYSIS_MODEL_NAME=your_analysis_model_name

# =============================================================================
# Python Code Execution Sandbox
# =============================================================================

# SandboxFusion endpoints for Python interpreter
# Example: "http://22.16.67.220:8080,http://22.16.78.153:8080,http://22.17.10.216:8080"
# See: https://github.com/bytedance/SandboxFusion
SANDBOX_FUSION_ENDPOINT=your_sandbox_endpoint
TORCH_COMPILE_CACHE_DIR=./cache

# =============================================================================
# IDP Service (Advanced File Parsing - Optional)
# =============================================================================

# IDP service for enhanced file parsing
# Set USE_IDP=True and provide credentials for more powerful parsing
# Documentation: https://help.aliyun.com/zh/document-mind/developer-reference/use-idp-llm-to-complete-document-summary
USE_IDP=False
IDP_KEY_ID=your_idp_key_id
IDP_KEY_SECRET=your_idp_key_secret

# =============================================================================
# Multi-Worker Configuration (Optional)
# =============================================================================

# These are typically set by distributed training frameworks
# WORLD_SIZE=1
# RANK=0

# =============================================================================
# Prompts Configuration (Optional)
# =============================================================================
# You can customize the system prompt and extractor prompt here
# If not set, default prompts will be used
# SYSTEM_PROMPT="Your custom system prompt here"
# EXTRACTOR_PROMPT="Your custom extractor prompt here"

# =============================================================================
# LLM Call Limit Configuration
# =============================================================================
# Maximum number of LLM calls per inference run
MAX_LLM_CALL_PER_RUN=100